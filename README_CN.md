# Fleming-VL

<p align="center">
    ğŸŒ <a href="" target="_blank">åšå®¢</a> | ğŸ“ƒ <a href="" target="_blank">è®ºæ–‡</a> | ğŸ¤— <a href="https://huggingface.co/UbiquantAI/Fleming-VL-8B" target="_blank">æ¨¡å‹</a> |  ğŸ¤— <a href="" target="_blank">è®­ç»ƒæ•°æ®</a>  |  ğŸ¥ <a href="" target="_blank">æ¼”ç¤º</a>

</p>


## ğŸ“– æ¨¡å‹æ¦‚è¿°

Fleming-VL æ˜¯ä¸€ä¸ªé¢å‘åŒ»ç–—åœºæ™¯çš„å¤šæ¨¡æ€æ¨ç†æ¨¡å‹ï¼Œèƒ½å¤Ÿå¤„ç†å’Œåˆ†æå„ç§ç±»å‹çš„åŒ»ç–—æ•°æ®ï¼ŒåŒ…æ‹¬2Då›¾åƒã€3Dä½“ç§¯æ•°æ®å’Œè§†é¢‘åºåˆ—ã€‚è¯¥æ¨¡å‹å¯¹å¤æ‚çš„å¤šæ¨¡æ€åŒ»ç–—é—®é¢˜è¿›è¡Œé€æ­¥åˆ†æï¼Œå¹¶äº§ç”Ÿå¯é çš„ç­”æ¡ˆã€‚åŸºäº GRPO æ¨ç†èŒƒå¼ï¼ŒFleming-VL æ‰©å±•äº†å¤„ç†å¤šæ ·åŒ–åŒ»å­¦å½±åƒæ¨¡æ€çš„èƒ½åŠ›ï¼ŒåŒæ—¶ä¿æŒå¼ºå¤§çš„æ¨ç†æ€§èƒ½ã€‚

**æ¨¡å‹ç‰¹ç‚¹ï¼š**

* **å¤šæ¨¡æ€å¤„ç†** æ”¯æŒå„ç§åŒ»ç–—æ•°æ®ç±»å‹ï¼ŒåŒ…æ‹¬2Då›¾åƒï¼ˆXå…‰ç‰‡ã€ç—…ç†åˆ‡ç‰‡ï¼‰ã€3Dä½“ç§¯ï¼ˆCT/MRIæ‰«æï¼‰å’Œè§†é¢‘ï¼ˆè¶…å£°ã€å†…çª¥é•œã€æ‰‹æœ¯è®°å½•ï¼‰ï¼›
* **åŒ»å­¦æ¨ç†** æ‰§è¡Œé€æ­¥çš„æ€ç»´é“¾æ¨ç†æ¥åˆ†æå¤æ‚çš„åŒ»å­¦é—®é¢˜ï¼Œç»“åˆè§†è§‰ä¿¡æ¯å’ŒåŒ»å­¦çŸ¥è¯†æä¾›å¯é çš„è¯Šæ–­è§è§£ã€‚

## ğŸ“¦ å‘å¸ƒç‰ˆæœ¬

- **Fleming-VL-7B** â€”â€” åŸºäº InternVL3-8B è®­ç»ƒ  
  ğŸ¤— [`UbiquantAI/Fleming-VL-8B`](https://huggingface.co/UbiquantAI/Fleming-VL-8B)
- **Fleming-VL-38B** â€”â€” åŸºäº InternVL3-38B è®­ç»ƒ   
  ğŸ¤— [`UbiquantAI/Fleming-VL-8B`](https://huggingface.co/UbiquantAI/Fleming-VL-38B)

## ğŸ“Š æ€§èƒ½è¡¨ç°

<div align="center">
  <figure>
    <img src="images/main_benchmark.png" alt="Main Benchmark Results" width="60%">
    <figcaption><b>å›¾ 1.</b> ä¸»è¦åŸºå‡†æµ‹è¯•ç»“æœã€‚</figcaption>
  </figure>
</div>

<div align="center">
  <figure>
    <img src="images/vqa.png" alt="General Medical Vqa" width="60%">
    <figcaption><b>å›¾ 2.</b> é€šç”¨åŒ»å­¦ VQAã€‚</figcaption>
  </figure>
</div>

<div align="center">
  <figure>
    <img src="images/report.png" alt="Medical Report Generation" width="60%">
    <figcaption><b>å›¾ 3.</b> åŒ»å­¦æŠ¥å‘Šç”Ÿæˆã€‚</figcaption>
  </figure>
</div>

<div align="center">
  <figure>
    <img src="images/video_3d.png" alt="Video and 3D understanding" width="60%">
    <figcaption><b>å›¾ 4.</b> è§†é¢‘å’Œ3Dç†è§£ã€‚</figcaption>
  </figure>
</div>

## ğŸ“Š åŒ»å­¦å¤šæ¨¡æ€ä»»åŠ¡

<div align="center">
  <figure>
    <img src="images/vis.jpg" alt="Visualization" width="100%">
    <figcaption><b>è·¨ä¸åŒåŒ»å­¦å¤šæ¨¡æ€ä»»åŠ¡çš„æ¨ç†</b></figcaption>
  </figure>
</div>

<br>

### ğŸ¯ ä¸»è¦åŒ»å­¦å¤šæ¨¡æ€ä»»åŠ¡

<div align="center">

| **ä»»åŠ¡** | **æè¿°** |
|:---|:---|
| ğŸ¥ **ç–¾ç—…è¯Šæ–­** | æ•´åˆåŒ»å­¦å½±åƒä¸ä¸´åºŠè®°å½•ï¼Œå®ç°å…¨é¢çš„ç–¾ç—…è¯†åˆ« |
| ğŸ”„ **è·¨æ¨¡æ€åˆ†æ** | èåˆä¸åŒå½±åƒæ¨¡æ€ï¼Œæå‡è¯Šæ–­å‡†ç¡®æ€§ |
| ğŸ“„ **åŒ»å­¦æŠ¥å‘Šç”Ÿæˆ** | ä»åŒ»å­¦å½±åƒè‡ªåŠ¨ç”Ÿæˆè¯Šæ–­æŠ¥å‘Š |
| ğŸ’Š **æ²»ç–—ååº”é¢„æµ‹** | åˆ©ç”¨å¤šæ¨¡æ€æ•°æ®é¢„æµ‹æ‚£è€…å¯¹æ²»ç–—çš„ååº” |
| ğŸ’¬ **ä¸´åºŠé—®ç­”** | æ•´åˆå½±åƒä¸è‡ªç„¶è¯­è¨€æŸ¥è¯¢çš„AIé—®ç­”ç³»ç»Ÿ |

</div>

## ğŸ”§ å¿«é€Ÿå¼€å§‹

```python
"""
Fleming-VL-8B Multi-Modal Inference Script

This script demonstrates three inference modes:
1. Single image inference
2. Video inference (frame-by-frame)
3. 3D medical image (CT/MRI) inference from .npy files

Model: UbiquantAI/Fleming-VL-8B
Based on: InternVL_chat-1.2 template
"""

from transformers import AutoTokenizer, AutoModel, CLIPImageProcessor
from decord import VideoReader, cpu
from PIL import Image
import numpy as np
import shutil
import torch
import os


# ============================================================================
# Configuration
# ============================================================================

MODEL_PATH = "UbiquantAI/Fleming-VL-8B"
REQUIRED_FILES_DIR = './required_files'

# Prompt template for reasoning-based responses
REASONING_PROMPT = (
    "A conversation between User and Assistant. The user asks a question, "
    "and the Assistant solves it. The assistant first thinks about the "
    "reasoning process in the mind and then provides the user a concise "
    "final answer in a short word or phrase. The reasoning process and "
    "answer are enclosed within <think> </think> and <answer> </answer> "
    "tags, respectively, i.e., <think> reasoning process here </think>"
    "<answer> answer here </answer>"
)


# ============================================================================
# Utility Functions
# ============================================================================

def copy_necessary_files(target_path, source_path):
    """
    Copy required model configuration files to the model directory.
    
    Args:
        target_path: Destination directory (model path)
        source_path: Source directory containing required files
    """
    required_files = [
        "modeling_internvl_chat.py",
        "conversation.py",
        "modeling_intern_vit.py",
        "preprocessor_config.json",
        "configuration_internvl_chat.py",
        "configuration_intern_vit.py",
    ]
    
    for filename in required_files:
        target_file = os.path.join(target_path, filename)
        source_file = os.path.join(source_path, filename)
        
        if not os.path.exists(target_file):
            print(f"File {filename} not found in target path, copying from source...")
            
            if os.path.exists(source_file):
                try:
                    shutil.copy2(source_file, target_file)
                    print(f"Successfully copied {filename}")
                except Exception as e:
                    print(f"Error copying {filename}: {str(e)}")
            else:
                print(f"Warning: Source file {filename} does not exist, cannot copy")
        else:
            print(f"File {filename} already exists")


def load_model(model_path, use_flash_attn=True):
    """
    Load the vision-language model and tokenizer.
    
    Args:
        model_path: Path to the pretrained model
        use_flash_attn: Whether to use flash attention (default: True)
    
    Returns:
        tuple: (model, tokenizer)
    """
    model = AutoModel.from_pretrained(
        model_path,
        torch_dtype=torch.bfloat16,
        low_cpu_mem_usage=True,
        use_flash_attn=use_flash_attn,
        trust_remote_code=True
    ).eval().cuda()
    
    tokenizer = AutoTokenizer.from_pretrained(
        model_path,
        trust_remote_code=True,
        use_fast=False
    )
    
    return model, tokenizer


# ============================================================================
# Image Inference
# ============================================================================

def inference_single_image(model, tokenizer, image_path, question, prompt=REASONING_PROMPT):
    """
    Perform inference on a single image.
    
    Args:
        model: Loaded vision-language model
        tokenizer: Loaded tokenizer
        image_path: Path to the input image
        question: Question to ask about the image
        prompt: System prompt template
    
    Returns:
        str: Model response
    """
    # Load and preprocess image
    image_processor = CLIPImageProcessor.from_pretrained(MODEL_PATH)
    image = Image.open(image_path).resize((448, 448))
    pixel_values = image_processor(
        images=image,
        return_tensors='pt'
    ).pixel_values.to(torch.bfloat16).cuda()
    
    # Prepare question with prompt and image token
    full_question = f"{prompt}\n<image>\n{question}"
    
    # Generate response
    generation_config = dict(max_new_tokens=1024, do_sample=False)
    response = model.chat(tokenizer, pixel_values, full_question, generation_config)
    
    return response


# ============================================================================
# Video Inference
# ============================================================================

def get_frame_indices(bound, fps, max_frame, first_idx=0, num_segments=32):
    """
    Calculate evenly distributed frame indices for video sampling.
    
    Args:
        bound: Tuple of (start_time, end_time) in seconds, or None for full video
        fps: Frames per second of the video
        max_frame: Maximum frame index
        first_idx: First frame index to consider
        num_segments: Number of frames to sample
    
    Returns:
        np.array: Array of frame indices
    """
    if bound:
        start, end = bound[0], bound[1]
    else:
        start, end = -100000, 100000
    
    start_idx = max(first_idx, round(start * fps))
    end_idx = min(round(end * fps), max_frame)
    seg_size = float(end_idx - start_idx) / num_segments
    
    frame_indices = np.array([
        int(start_idx + (seg_size / 2) + np.round(seg_size * idx))
        for idx in range(num_segments)
    ])
    
    return frame_indices


def load_video(video_path, model_path, bound=None, num_segments=32):
    """
    Load frames from a video file.
    
    Args:
        video_path: Path to the video file
        model_path: Path to the model (for image processor)
        bound: Optional time bounds (start, end) in seconds
        num_segments: Number of frames to extract (default: 32)
    
    Returns:
        tuple: (pixel_values tensor, num_patches_list) or None if error
    """
    try:
        video_reader = VideoReader(video_path, num_threads=1, ctx=cpu(0))
        max_frame = len(video_reader) - 1
        fps = float(video_reader.get_avg_fps())
        
        # Get frame indices to sample
        frame_indices = get_frame_indices(bound, fps, max_frame, first_idx=0, num_segments=num_segments)
        
        image_processor = CLIPImageProcessor.from_pretrained(model_path)
        pixel_values_list = []
        num_patches_list = []
        
        # Process each frame
        for frame_index in frame_indices:
            img = Image.fromarray(video_reader[frame_index].asnumpy()).convert('RGB')
            pixel_values = image_processor(images=img, return_tensors='pt').pixel_values
            num_patches_list.append(pixel_values.shape[0])
            pixel_values_list.append(pixel_values)
        
        pixel_values = torch.cat(pixel_values_list)
        return pixel_values, num_patches_list
    
    except Exception as e:
        print(f"Error processing video: {str(e)}")
        return None


def inference_video(model, tokenizer, video_path, video_duration, question, prompt=REASONING_PROMPT):
    """
    Perform inference on a video.
    
    Args:
        model: Loaded vision-language model
        tokenizer: Loaded tokenizer
        video_path: Path to the video file
        video_duration: Duration of video in seconds
        question: Question to ask about the video
        prompt: System prompt template
    
    Returns:
        str: Model response or None if error
    """
    # Load video frames
    result = load_video(video_path, MODEL_PATH, bound=(0, video_duration))
    
    if result is None:
        return None
    
    pixel_values, num_patches_list = result
    pixel_values = pixel_values.to(torch.bfloat16).cuda()
    
    # Create image token prefix for all frames
    image_prefix = ''.join([f'<image>\n' for _ in range(len(num_patches_list))])
    
    # Prepare question with prompt and image tokens
    full_question = f"{prompt}\n{image_prefix}{question}"
    
    # Generate response
    generation_config = dict(max_new_tokens=1024, do_sample=False)
    response, history = model.chat(
        tokenizer,
        pixel_values,
        full_question,
        generation_config,
        num_patches_list=num_patches_list,
        history=None,
        return_history=True
    )
    
    return response


# ============================================================================
# 3D Medical Image Inference
# ============================================================================

def normalize_image(image):
    """
    Normalize image array to 0-255 range.
    
    Args:
        image: NumPy array of image data
    
    Returns:
        np.array: Normalized image as uint8
    """
    img_min = np.min(image)
    img_max = np.max(image)
    
    if img_max - img_min == 0:
        return np.zeros_like(image, dtype=np.uint8)
    
    return ((image - img_min) / (img_max - img_min) * 255).astype(np.uint8)


def convert_npy_to_images(npy_path, model_path, num_slices=11):
    """
    Convert 3D medical image (.npy) to multiple 2D RGB images.
    
    Expected input shape: (32, 256, 256) or (1, 32, 256, 256)
    Extracts evenly distributed slices and converts to RGB format.
    
    Args:
        npy_path: Path to the .npy file
        model_path: Path to the model (for image processor)
        num_slices: Number of slices to extract (default: 11)
    
    Returns:
        tuple: (pixel_values tensor, list of num_patches per slice) or False if error
    """
    try:
        # Load .npy file
        data = np.load(npy_path)
        
        # Handle shape (1, 32, 256, 256) -> (32, 256, 256)
        if data.ndim == 4 and data.shape[0] == 1:
            data = data[0]
        
        # Validate shape
        if data.shape != (32, 256, 256):
            print(f"Warning: {npy_path} has shape {data.shape}, expected (32, 256, 256), skipping")
            return False
        
        # Select evenly distributed slices from 32 slices
        indices = np.linspace(0, 31, num_slices, dtype=int)
        
        image_processor = CLIPImageProcessor.from_pretrained(model_path)
        pixel_values_list = []
        num_patches_list = []
        
        # Process each selected slice
        for idx in indices:
            # Get slice
            slice_img = data[idx]
            
            # Normalize to 0-255
            normalized = normalize_image(slice_img)
            
            # Convert grayscale to RGB by stacking
            rgb_img = np.stack([normalized, normalized, normalized], axis=-1)
            
            # Convert to PIL Image
            img = Image.fromarray(rgb_img)
            
            # Preprocess with CLIP processor
            pixel_values = image_processor(images=img, return_tensors='pt').pixel_values
            num_patches_list.append(pixel_values.shape[0])
            pixel_values_list.append(pixel_values)
        
        pixel_values = torch.cat(pixel_values_list)
        return pixel_values, num_patches_list
    
    except Exception as e:
        print(f"Error processing {npy_path}: {str(e)}")
        return False


def inference_3d_medical_image(model, tokenizer, npy_path, question, prompt=REASONING_PROMPT):
    """
    Perform inference on 3D medical images stored as .npy files.
    
    Args:
        model: Loaded vision-language model
        tokenizer: Loaded tokenizer
        npy_path: Path to the .npy file (shape: 32x256x256)
        question: Question to ask about the image
        prompt: System prompt template
    
    Returns:
        str: Model response or None if error
    """
    # Convert 3D volume to multiple 2D slices
    result = convert_npy_to_images(npy_path, MODEL_PATH)
    
    if result is False:
        return None
    
    pixel_values, num_patches_list = result
    pixel_values = pixel_values.to(torch.bfloat16).cuda()
    
    # Create image token prefix for all slices
    image_prefix = ''.join([f'<image>\n' for _ in range(len(num_patches_list))])
    
    # Prepare question with prompt and image tokens
    full_question = f"{prompt}\n{image_prefix}{question}"
    
    # Generate response
    generation_config = dict(max_new_tokens=1024, do_sample=False)
    response, history = model.chat(
        tokenizer,
        pixel_values,
        full_question,
        generation_config,
        num_patches_list=num_patches_list,
        history=None,
        return_history=True
    )
    
    return response


# ============================================================================
# Main Execution Examples
# ============================================================================

def main():
    """
    Main function demonstrating all three inference modes.
    """
    # Copy necessary files
    copy_necessary_files(MODEL_PATH, REQUIRED_FILES_DIR)
    
    # ========================================================================
    # Example 1: Single Image Inference
    # ========================================================================
    print("\n" + "="*80)
    print("EXAMPLE 1: Single Image Inference")
    print("="*80)
    
    image_path = "demo/resource/test.png"
    question = (
        "What imaging technique was employed to obtain this picture?\n"
        "A. PET scan. B. CT scan. C. Blood test. D. Fundus imaging."
    )
    
    model, tokenizer = load_model(MODEL_PATH, use_flash_attn=True)
    response = inference_single_image(model, tokenizer, image_path, question)
    
    print(f"\nUser: {question}")
    print(f"Assistant: {response}")
    
    # Clean up GPU memory
    del model, tokenizer
    torch.cuda.empty_cache()
    
    # ========================================================================
    # Example 2: Video Inference
    # ========================================================================
    print("\n" + "="*80)
    print("EXAMPLE 2: Video Inference")
    print("="*80)
    
    video_path = "demo/resource/test.mp4"
    video_duration = 6  # seconds
    question = "Please describe the video."
    
    model, tokenizer = load_model(MODEL_PATH, use_flash_attn=False)
    response = inference_video(model, tokenizer, video_path, video_duration, question)
    
    print(f"\nUser: {question}")
    print(f"Assistant: {response}")
    
    # Clean up GPU memory
    del model, tokenizer
    torch.cuda.empty_cache()
    
    # ========================================================================
    # Example 3: 3D Medical Image Inference
    # ========================================================================
    print("\n" + "="*80)
    print("EXAMPLE 3: 3D Medical Image Inference")
    print("="*80)
    
    npy_path = "demo/resource/test.npy"
    question = "What device is observed on the chest wall?"
    
    # Example cases:
    # Case 1: /path/to/test_1016_d_2.npy
    #   Question: "Where is the largest lymph node observed?"
    #   Answer: "Right hilar region."
    #
    # Case 2: /path/to/test_1031_a_2.npy
    #   Question: "What device is observed on the chest wall?"
    #   Answer: "Pacemaker."
    
    model, tokenizer = load_model(MODEL_PATH, use_flash_attn=False)
    response = inference_3d_medical_image(model, tokenizer, npy_path, question)
    
    if response:
        print(f"\nUser: {question}")
        print(f"Assistant: {response}")
    else:
        print("\nError: Failed to process 3D medical image")
    
    # Clean up GPU memory
    del model, tokenizer
    torch.cuda.empty_cache()


if __name__ == "__main__":
    main()
```


## âš ï¸ å®‰å…¨å£°æ˜

æœ¬é¡¹ç›®ä»…ä¾›ç ”ç©¶å’Œéä¸´åºŠå‚è€ƒä½¿ç”¨ï¼›ä¸å¾—ç”¨äºå®é™…è¯Šæ–­æˆ–æ²»ç–—å†³ç­–ã€‚  
ç”Ÿæˆçš„æ¨ç†è½¨è¿¹æ˜¯å¯å®¡è®¡çš„ä¸­é—´è¿‡ç¨‹ï¼Œä¸æ„æˆåŒ»ç–—å»ºè®®ã€‚  
åœ¨åŒ»ç–—åœºæ™¯ä¸­ï¼Œç»“æœå¿…é¡»ç”±åˆæ ¼çš„ä¸“ä¸šäººå‘˜å®¡æŸ¥å’Œæ‰¹å‡†ï¼Œå¹¶ä¸”å¿…é¡»éµå®ˆæ‚¨æ‰€åœ¨åœ°åŒºçš„æ‰€æœ‰é€‚ç”¨æ³•å¾‹ã€æ³•è§„å’Œéšç§åˆè§„è¦æ±‚ã€‚

## ğŸ“š å¼•ç”¨

```bibtex
@misc{flemingr1,
      title={Fleming-R1: Toward Expert-Level Medical Reasoning via Reinforcement Learning}, 
      author={Chi Liu and Derek Li and Yan Shu and Robin Chen and Derek Duan and Teng Fang and Bryan Dai},
      year={2025},
      eprint={2509.15279},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2509.15279}, 
}
```
